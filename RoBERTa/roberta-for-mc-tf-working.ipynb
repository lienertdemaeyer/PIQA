{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nfrom datasets import load_dataset\nfrom transformers import RobertaTokenizerFast, TFRobertaForMultipleChoice, create_optimizer\nfrom transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\nfrom dataclasses import dataclass\nfrom typing import Optional, Union\nimport numpy as np","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-14T00:54:58.686040Z","iopub.execute_input":"2024-08-14T00:54:58.686420Z","iopub.status.idle":"2024-08-14T00:54:58.693535Z","shell.execute_reply.started":"2024-08-14T00:54:58.686388Z","shell.execute_reply":"2024-08-14T00:54:58.692575Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Load the PIQA dataset\nprint(\"Loading PIQA dataset...\")\npiqa_dataset = load_dataset(\"piqa\")\nprint(\"Dataset loaded. Sample:\")\nprint(piqa_dataset['train'][:3])","metadata":{"execution":{"iopub.status.busy":"2024-08-14T00:55:08.473474Z","iopub.execute_input":"2024-08-14T00:55:08.473853Z","iopub.status.idle":"2024-08-14T00:55:10.257622Z","shell.execute_reply.started":"2024-08-14T00:55:08.473822Z","shell.execute_reply":"2024-08-14T00:55:10.256724Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Loading PIQA dataset...\nDataset loaded. Sample:\n{'goal': [\"When boiling butter, when it's ready, you can\", 'To permanently attach metal legs to a chair, you can', 'how do you indent something?'], 'sol1': ['Pour it onto a plate', 'Weld the metal together to get it to stay firmly in place', 'leave a space before starting the writing'], 'sol2': ['Pour it into a jar', 'Nail the metal together to get it to stay firmly in place', 'press the spacebar'], 'label': [1, 0, 0]}\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load model and tokenizer\nprint(\"\\nLoading RoBERTa model and tokenizer...\")\nmodel_checkpoint = \"roberta-base\"\ntokenizer = RobertaTokenizerFast.from_pretrained(model_checkpoint)\nmodel = TFRobertaForMultipleChoice.from_pretrained(model_checkpoint)","metadata":{"execution":{"iopub.status.busy":"2024-08-14T00:55:18.905584Z","iopub.execute_input":"2024-08-14T00:55:18.906206Z","iopub.status.idle":"2024-08-14T00:55:20.521826Z","shell.execute_reply.started":"2024-08-14T00:55:18.906171Z","shell.execute_reply":"2024-08-14T00:55:20.520799Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"\nLoading RoBERTa model and tokenizer...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaForMultipleChoice: ['roberta.embeddings.position_ids']\n- This IS expected if you are initializing TFRobertaForMultipleChoice from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing TFRobertaForMultipleChoice from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights or buffers of the TF 2.0 model TFRobertaForMultipleChoice were not initialized from the PyTorch model and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define preprocessing function\ndef preprocess_function(examples):\n    first_sentences = [[context] * 2 for context in examples[\"goal\"]]\n    second_sentences = [[sol1, sol2] for sol1, sol2 in zip(examples[\"sol1\"], examples[\"sol2\"])]\n    \n    # Flatten everything\n    first_sentences = sum(first_sentences, [])\n    second_sentences = sum(second_sentences, [])\n    \n    # Tokenize\n    tokenized_examples = tokenizer(first_sentences, second_sentences, padding=False, truncation=True)\n    \n    # Un-flatten\n    result = {\n        k: [v[i : i + 2] for i in range(0, len(v), 2)]\n        for k, v in tokenized_examples.items()\n    }\n    \n    # Add labels\n    if \"label\" in examples:\n        result[\"labels\"] = examples[\"label\"]\n    \n    return result","metadata":{"execution":{"iopub.status.busy":"2024-08-14T00:58:49.362707Z","iopub.execute_input":"2024-08-14T00:58:49.363295Z","iopub.status.idle":"2024-08-14T00:58:49.370554Z","shell.execute_reply.started":"2024-08-14T00:58:49.363264Z","shell.execute_reply":"2024-08-14T00:58:49.369579Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# Preprocess the dataset\nprint(\"Preprocessing the dataset...\")\nencoded_datasets = piqa_dataset.map(preprocess_function, batched=True, remove_columns=piqa_dataset[\"train\"].column_names)\n\n# Data collator\n@dataclass\nclass DataCollatorForMultipleChoice:\n    tokenizer: PreTrainedTokenizerBase\n    padding: Union[bool, str, PaddingStrategy] = True\n    max_length: Optional[int] = None\n    pad_to_multiple_of: Optional[int] = None\n\n    def __call__(self, features):\n        label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n        labels = [feature.pop(label_name) for feature in features]\n        batch_size = len(features)\n        num_choices = len(features[0][\"input_ids\"])\n        flattened_features = [\n            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)]\n            for feature in features\n        ]\n        flattened_features = sum(flattened_features, [])\n\n        batch = self.tokenizer.pad(\n            flattened_features,\n            padding=self.padding,\n            max_length=self.max_length,\n            pad_to_multiple_of=self.pad_to_multiple_of,\n            return_tensors=\"np\",\n        )\n\n        # Un-flatten\n        batch = {\n            k: tf.reshape(v, (batch_size, num_choices, -1)) for k, v in batch.items()\n        }\n        # Add back labels\n        batch[\"labels\"] = tf.convert_to_tensor(labels, dtype=tf.int64)\n        return batch","metadata":{"execution":{"iopub.status.busy":"2024-08-14T00:58:52.134724Z","iopub.execute_input":"2024-08-14T00:58:52.135818Z","iopub.status.idle":"2024-08-14T00:58:55.724055Z","shell.execute_reply.started":"2024-08-14T00:58:52.135781Z","shell.execute_reply":"2024-08-14T00:58:55.723108Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Preprocessing the dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/16113 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"262a4fc651834a2380bdeb148382066a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3084 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"607c03ea217e4907a22e3fa966179854"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1838 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16bfbb5a92264ee2bf4cc1425e61fa90"}},"metadata":{}}]},{"cell_type":"code","source":"# Preprocess the dataset\nprint(\"Preprocessing the dataset...\")\nencoded_datasets = piqa_dataset.map(preprocess_function, batched=True, remove_columns=piqa_dataset[\"train\"].column_names)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-14T00:59:00.639202Z","iopub.execute_input":"2024-08-14T00:59:00.639574Z","iopub.status.idle":"2024-08-14T00:59:00.741232Z","shell.execute_reply.started":"2024-08-14T00:59:00.639542Z","shell.execute_reply":"2024-08-14T00:59:00.740266Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"Preprocessing the dataset...\n","output_type":"stream"}]},{"cell_type":"code","source":"# Data collator\n@dataclass\nclass DataCollatorForMultipleChoice:\n    tokenizer: PreTrainedTokenizerBase\n    padding: Union[bool, str, PaddingStrategy] = True\n    max_length: Optional[int] = None\n    pad_to_multiple_of: Optional[int] = None\n\n    def __call__(self, features):\n        label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n        labels = [feature.pop(label_name) for feature in features]\n        batch_size = len(features)\n        num_choices = len(features[0][\"input_ids\"])\n        flattened_features = [\n            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)]\n            for feature in features\n        ]\n        flattened_features = sum(flattened_features, [])\n\n        batch = self.tokenizer.pad(\n            flattened_features,\n            padding=self.padding,\n            max_length=self.max_length,\n            pad_to_multiple_of=self.pad_to_multiple_of,\n            return_tensors=\"np\",\n        )\n\n        # Un-flatten\n        batch = {\n            k: tf.reshape(v, (batch_size, num_choices, -1)) for k, v in batch.items()\n        }\n        # Add back labels\n        batch[\"labels\"] = tf.convert_to_tensor(labels, dtype=tf.int64)\n        return batch","metadata":{"execution":{"iopub.status.busy":"2024-08-14T00:59:12.822127Z","iopub.execute_input":"2024-08-14T00:59:12.822953Z","iopub.status.idle":"2024-08-14T00:59:12.835482Z","shell.execute_reply.started":"2024-08-14T00:59:12.822895Z","shell.execute_reply":"2024-08-14T00:59:12.834480Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# Set up data collator\ndata_collator = DataCollatorForMultipleChoice(tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-08-14T00:59:15.357696Z","iopub.execute_input":"2024-08-14T00:59:15.358140Z","iopub.status.idle":"2024-08-14T00:59:15.362535Z","shell.execute_reply.started":"2024-08-14T00:59:15.358107Z","shell.execute_reply":"2024-08-14T00:59:15.361390Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"# Split the data into train and validation sets\nprint(\"Splitting dataset into train and validation sets...\")\ntrain_test_split = encoded_datasets[\"train\"].train_test_split(test_size=0.1, seed=42)\nencoded_datasets_2 = {\n    \"train\": train_test_split[\"train\"],\n    \"test\": train_test_split[\"test\"],\n    \"validation\": encoded_datasets[\"validation\"]\n}\n\n# Prepare datasets with a smaller batch size\nprint(\"Preparing datasets for training...\")\nbatch_size = 4  # Reduced batch size\n\n\ntrain_set = model.prepare_tf_dataset(\n    encoded_datasets_2['train'],\n    shuffle=True,\n    batch_size=batch_size,\n    collate_fn=data_collator,\n)\n\nval_set = model.prepare_tf_dataset(\n    encoded_datasets_2['test'],\n    shuffle=False,\n    batch_size=batch_size,\n    collate_fn=data_collator,\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-14T01:08:29.931327Z","iopub.execute_input":"2024-08-14T01:08:29.932062Z","iopub.status.idle":"2024-08-14T01:08:30.200634Z","shell.execute_reply.started":"2024-08-14T01:08:29.932026Z","shell.execute_reply":"2024-08-14T01:08:30.199799Z"},"trusted":true},"execution_count":38,"outputs":[{"name":"stdout","text":"Splitting dataset into train and validation sets...\nPreparing datasets for training...\n","output_type":"stream"}]},{"cell_type":"code","source":"# Model parameters\nlearning_rate = 2e-5\nnum_train_epochs = 3\nweight_decay = 0.01\n\n# Prepare optimizer with gradient accumulation\nnum_train_steps = len(train_set) * num_train_epochs\naccumulation_steps = 4\noptimizer, lr_schedule = create_optimizer(\n    init_lr=learning_rate,\n    num_warmup_steps=0,\n    num_train_steps=num_train_steps // accumulation_steps,\n    weight_decay_rate=weight_decay\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-14T01:08:32.760240Z","iopub.execute_input":"2024-08-14T01:08:32.761012Z","iopub.status.idle":"2024-08-14T01:08:32.767067Z","shell.execute_reply.started":"2024-08-14T01:08:32.760977Z","shell.execute_reply":"2024-08-14T01:08:32.765942Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"# Compile the model\nprint(\"Compiling the model...\")\nmodel.compile(optimizer=optimizer, metrics=[\"accuracy\"])\n\n# Train the model\nprint(\"Starting model training...\")\nhistory = model.fit(\n    train_set,\n    validation_data=val_set,\n    epochs=num_train_epochs\n)\n\nprint(\"Training complete!\")","metadata":{"execution":{"iopub.status.busy":"2024-08-14T01:09:01.275353Z","iopub.execute_input":"2024-08-14T01:09:01.276094Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Compiling the model...\nStarting model training...\nEpoch 1/3\nWARNING: AutoGraph could not transform <function create_autocast_variable at 0x794a8c9d67a0> and will run it as-is.\nPlease report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\nCause: <gast.gast.Expr object at 0x794610156b90>\nTo silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n 749/3625 [=====>........................] - ETA: 13:34 - loss: 0.6953 - accuracy: 0.4930","output_type":"stream"}]}]}