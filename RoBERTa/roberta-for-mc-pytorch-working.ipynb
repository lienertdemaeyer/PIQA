{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom datasets import load_dataset\nfrom transformers import (\n    RobertaTokenizerFast,\n    RobertaForMultipleChoice,\n    TrainingArguments,\n    Trainer,\n)\nimport numpy as np\nfrom dataclasses import dataclass\nfrom typing import Optional, Union\nfrom transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-14T01:34:17.563851Z","iopub.execute_input":"2024-08-14T01:34:17.564484Z","iopub.status.idle":"2024-08-14T01:34:35.582589Z","shell.execute_reply.started":"2024-08-14T01:34:17.564433Z","shell.execute_reply":"2024-08-14T01:34:35.581830Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-08-14 01:34:25.180154: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-14 01:34:25.180273: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-14 01:34:25.317710: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"# Check if CUDA is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-14T01:34:37.119550Z","iopub.execute_input":"2024-08-14T01:34:37.120210Z","iopub.status.idle":"2024-08-14T01:34:37.149670Z","shell.execute_reply.started":"2024-08-14T01:34:37.120178Z","shell.execute_reply":"2024-08-14T01:34:37.148589Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load the PIQA dataset\nprint(\"Loading PIQA dataset...\")\npiqa_dataset = load_dataset(\"piqa\")\nprint(\"Dataset loaded. Sample:\")\nprint(piqa_dataset['train'][:3])","metadata":{"execution":{"iopub.status.busy":"2024-08-14T01:34:39.848361Z","iopub.execute_input":"2024-08-14T01:34:39.848754Z","iopub.status.idle":"2024-08-14T01:34:48.039790Z","shell.execute_reply.started":"2024-08-14T01:34:39.848724Z","shell.execute_reply":"2024-08-14T01:34:48.038859Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Loading PIQA dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/5.36k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36072dc3bb6a4f41a4f762f9f10c036a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading readme:   0%|          | 0.00/8.41k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"46b72f57a9c542fea8b6e5dbd23ffb4b"}},"metadata":{}},{"output_type":"stream","name":"stdin","text":"The repository for piqa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/piqa.\nYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\nDo you wish to run the custom code? [y/N]  y\n"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/1.82M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"21430113cd8a4f39b3c3fb59a75954c9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/815k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"974fa864a8c040fcb19a26d126802e48"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/16113 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ace04de4f6d543c299655056f4c7b7ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/3084 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9fbc1f301cb4e82869b3bccf9700e07"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/1838 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"354f96a62f3e40b9ac1b4b2581957713"}},"metadata":{}},{"name":"stdout","text":"Dataset loaded. Sample:\n{'goal': [\"When boiling butter, when it's ready, you can\", 'To permanently attach metal legs to a chair, you can', 'how do you indent something?'], 'sol1': ['Pour it onto a plate', 'Weld the metal together to get it to stay firmly in place', 'leave a space before starting the writing'], 'sol2': ['Pour it into a jar', 'Nail the metal together to get it to stay firmly in place', 'press the spacebar'], 'label': [1, 0, 0]}\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load model and tokenizer\nprint(\"\\nLoading RoBERTa model and tokenizer...\")\nmodel_checkpoint = \"roberta-base\"\ntokenizer = RobertaTokenizerFast.from_pretrained(model_checkpoint)\nmodel = RobertaForMultipleChoice.from_pretrained(model_checkpoint).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-08-14T01:34:51.064035Z","iopub.execute_input":"2024-08-14T01:34:51.064855Z","iopub.status.idle":"2024-08-14T01:34:56.545218Z","shell.execute_reply.started":"2024-08-14T01:34:51.064816Z","shell.execute_reply":"2024-08-14T01:34:56.544207Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"\nLoading RoBERTa model and tokenizer...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ecf7b4092b94bae816953d2bac81f88"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"591ec61eb7a5491689d177614ef6adbf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a89bba3678f042d6ab238e8de0f43b39"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7252575aa9d945509ab02b93ab6681f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ad7f908e447468a89369ca2d706365d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d566570218941b5bf7b1349ddbe605e"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaForMultipleChoice were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define preprocessing function\ndef preprocess_function(examples):\n    first_sentences = [[context] * 2 for context in examples[\"goal\"]]\n    second_sentences = [[sol1, sol2] for sol1, sol2 in zip(examples[\"sol1\"], examples[\"sol2\"])]\n    \n    # Flatten everything\n    first_sentences = sum(first_sentences, [])\n    second_sentences = sum(second_sentences, [])\n    \n    # Tokenize\n    tokenized_examples = tokenizer(first_sentences, second_sentences, padding=False, truncation=True)\n    \n    # Un-flatten\n    result = {\n        k: [v[i : i + 2] for i in range(0, len(v), 2)]\n        for k, v in tokenized_examples.items()\n    }\n    \n    # Add labels\n    if \"label\" in examples:\n        result[\"labels\"] = examples[\"label\"]\n    \n    return result","metadata":{"execution":{"iopub.status.busy":"2024-08-14T01:35:01.313940Z","iopub.execute_input":"2024-08-14T01:35:01.314292Z","iopub.status.idle":"2024-08-14T01:35:01.321558Z","shell.execute_reply.started":"2024-08-14T01:35:01.314264Z","shell.execute_reply":"2024-08-14T01:35:01.320621Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Preprocess the dataset\nprint(\"Preprocessing the dataset...\")\nencoded_datasets = piqa_dataset.map(preprocess_function, batched=True, remove_columns=piqa_dataset[\"train\"].column_names)\n\n# Data collator\n@dataclass\nclass DataCollatorForMultipleChoice:\n    tokenizer: PreTrainedTokenizerBase\n    padding: Union[bool, str, PaddingStrategy] = True\n    max_length: Optional[int] = None\n    pad_to_multiple_of: Optional[int] = None\n\n    def __call__(self, features):\n        label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n        labels = [feature.pop(label_name) for feature in features]\n        batch_size = len(features)\n        num_choices = len(features[0][\"input_ids\"])\n        flattened_features = [\n            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)]\n            for feature in features\n        ]\n        flattened_features = sum(flattened_features, [])\n\n        batch = self.tokenizer.pad(\n            flattened_features,\n            padding=self.padding,\n            max_length=self.max_length,\n            pad_to_multiple_of=self.pad_to_multiple_of,\n            return_tensors=\"np\",\n        )\n\n        # Un-flatten\n        batch = {\n            k: tf.reshape(v, (batch_size, num_choices, -1)) for k, v in batch.items()\n        }\n        # Add back labels\n        batch[\"labels\"] = tf.convert_to_tensor(labels, dtype=tf.int64)\n        return batch","metadata":{"execution":{"iopub.status.busy":"2024-08-14T01:35:03.582446Z","iopub.execute_input":"2024-08-14T01:35:03.583108Z","iopub.status.idle":"2024-08-14T01:35:07.032961Z","shell.execute_reply.started":"2024-08-14T01:35:03.583076Z","shell.execute_reply":"2024-08-14T01:35:07.032019Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Preprocessing the dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/16113 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90e1502bafaf48d1a61bdf1a0aeab523"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3084 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14e5932dc6124647aa47daa2e0e663fa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1838 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c10c9bebe92d42eebd3374e15205af76"}},"metadata":{}}]},{"cell_type":"code","source":"# Preprocess the dataset\nprint(\"Preprocessing the dataset...\")\nencoded_datasets = piqa_dataset.map(preprocess_function, batched=True, remove_columns=piqa_dataset[\"train\"].column_names)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-14T01:35:10.153255Z","iopub.execute_input":"2024-08-14T01:35:10.153907Z","iopub.status.idle":"2024-08-14T01:35:12.813989Z","shell.execute_reply.started":"2024-08-14T01:35:10.153874Z","shell.execute_reply":"2024-08-14T01:35:12.813337Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Preprocessing the dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/16113 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2188cef448e545469f605e1ea93f91e6"}},"metadata":{}}]},{"cell_type":"code","source":"@dataclass\nclass DataCollatorForMultipleChoice:\n    tokenizer: PreTrainedTokenizerBase\n    padding: Union[bool, str, PaddingStrategy] = True\n    max_length: Optional[int] = None\n    pad_to_multiple_of: Optional[int] = None\n\n    def __call__(self, features):\n        label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n        labels = [feature.pop(label_name) for feature in features]\n        batch_size = len(features)\n        num_choices = len(features[0][\"input_ids\"])\n        flattened_features = [\n            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)]\n            for feature in features\n        ]\n        flattened_features = sum(flattened_features, [])\n\n        batch = self.tokenizer.pad(\n            flattened_features,\n            padding=self.padding,\n            max_length=self.max_length,\n            pad_to_multiple_of=self.pad_to_multiple_of,\n            return_tensors=\"pt\",\n        )\n\n        # Un-flatten\n        batch = {\n            k: v.view(batch_size, num_choices, -1) for k, v in batch.items()\n        }\n        # Add back labels\n        batch[\"labels\"] = torch.tensor(labels, dtype=torch.long)\n        return batch\n\n# Create an instance of the data collator\ndata_collator = DataCollatorForMultipleChoice(tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-08-14T01:35:15.928632Z","iopub.execute_input":"2024-08-14T01:35:15.929380Z","iopub.status.idle":"2024-08-14T01:35:15.942540Z","shell.execute_reply.started":"2024-08-14T01:35:15.929334Z","shell.execute_reply":"2024-08-14T01:35:15.941203Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Set up data collator\ndata_collator = DataCollatorForMultipleChoice(tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-08-14T01:35:18.753271Z","iopub.execute_input":"2024-08-14T01:35:18.754119Z","iopub.status.idle":"2024-08-14T01:35:18.757938Z","shell.execute_reply.started":"2024-08-14T01:35:18.754085Z","shell.execute_reply":"2024-08-14T01:35:18.757046Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\n# Split the data into train and validation sets\nprint(\"Splitting dataset into train and validation sets...\")\ntrain_test_split = encoded_datasets[\"train\"].train_test_split(test_size=0.1, seed=42)\nencoded_datasets_2 = {\n    \"train\": train_test_split[\"train\"],\n    \"test\": train_test_split[\"test\"],\n    \"validation\": encoded_datasets[\"validation\"]\n}\n\n# Prepare datasets for training\nprint(\"Preparing datasets for training...\")\nbatch_size = 4  # Reduced batch size\n\n# Create DataLoaders\ntrain_dataset = encoded_datasets_2['train']\nval_dataset = encoded_datasets_2['test']\n\ntrain_dataloader = DataLoader(\n    train_dataset, \n    shuffle=True, \n    batch_size=batch_size, \n    collate_fn=data_collator\n)\n\nval_dataloader = DataLoader(\n    val_dataset, \n    shuffle=False, \n    batch_size=batch_size, \n    collate_fn=data_collator\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-14T01:35:20.850669Z","iopub.execute_input":"2024-08-14T01:35:20.851553Z","iopub.status.idle":"2024-08-14T01:35:20.877269Z","shell.execute_reply.started":"2024-08-14T01:35:20.851518Z","shell.execute_reply":"2024-08-14T01:35:20.876437Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Splitting dataset into train and validation sets...\nPreparing datasets for training...\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load and preprocess the dataset\npiqa_dataset = load_dataset(\"piqa\")\nencoded_datasets = piqa_dataset.map(preprocess_function, batched=True, remove_columns=piqa_dataset[\"train\"].column_names)\n\n# Set up TrainingArguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results_roberta_base\",\n    remove_unused_columns=False,\n    learning_rate=1e-5,\n    per_device_train_batch_size=4,\n    per_device_eval_batch_size=4,\n    num_train_epochs=10,\n    weight_decay=0.1,\n    adam_beta1=0.9,\n    adam_beta2=0.98,\n    adam_epsilon=1e-6,\n    max_grad_norm=1.0,\n    warmup_steps=int(0.06 * (16113 * 10) / 16),\n    lr_scheduler_type=\"polynomial\",\n    logging_dir=\"./logs_roberta_base\",\n    logging_steps=100,\n    save_strategy=\"epoch\",\n    eval_strategy=\"epoch\",  # Changed from evaluation_strategy to eval_strategy\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",\n    greater_is_better=True,\n    fp16=torch.cuda.is_available(),\n    gradient_accumulation_steps=8,\n    seed=42,\n    dataloader_num_workers=4,\n    report_to=\"none\",\n)\n\n# Define compute_metrics function\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    predictions = predictions.argmax(axis=1)\n    return {\"accuracy\": (predictions == labels).astype(float).mean().item()}\n\n# Set up Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=encoded_datasets[\"train\"],\n    eval_dataset=encoded_datasets[\"validation\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\n\n# Train the model\nprint(\"Starting model training...\")\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-08-14T01:35:24.084971Z","iopub.execute_input":"2024-08-14T01:35:24.085320Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Starting model training...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nYou're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\nYou're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nYou're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\nYou're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='629' max='5030' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [ 629/5030 05:57 < 41:49, 1.75 it/s, Epoch 1.25/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>0</td>\n      <td>0.693200</td>\n      <td>0.692911</td>\n      <td>0.564744</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nYou're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\nYou're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\nYou're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\nYou're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nYou're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\nYou're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\nYou're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\nYou're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}]}]}